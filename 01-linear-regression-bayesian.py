# -*- coding: utf-8 -*-
"""LinearRegression-Bayesian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dwoXxWe5XeWvxB3YvoQAkP2UGSYmEAci
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
# %matplotlib inline

noise_NUM = 10
w0 = -0.3
w1 = 0.5

# func calculates y = w1 * x + w0
def func(x):
    return w1 * x + w0

# generate noise and add them to y
def generate_noise_data(func,noise_NUM,std_dev):
    x_n = np.linspace(0,1,noise_NUM)
    t_n = func(x_n)+ np.random.normal(scale=std_dev,size=noise_NUM)
    return x_n,t_n

# draw the 10 noises and the truth line
x_true = np.arange(0, 1.01, 0.01)
y_true = func(x_true)
x_n,t_n = generate_noise_data(func,noise_NUM,0.3)
plt.plot(x_true,y_true,color='lime',label="True line")
plt.scatter(x_n,t_n,marker='o',color='blue',label="noise")

plt.yticks( [-1.5, 0.0, 1.5] )
plt.xticks( [0.0, 0.5, 1.0] )
plt.xlim(-0.05, 1.05)
plt.ylim(-1.5, 1.5)
plt.legend()
plt.show()

def posterior(Phi, t, alpha, beta):

  # calculate posterior probability using the equation in Lecture 07, 13p.
  S_N_inv = alpha * np.eye(Phi.shape[1]) + beta * Phi.T.dot(Phi)
  S_N = np.linalg.inv(S_N_inv)
  m_N = beta * S_N.dot(Phi.T).dot(t)

  return m_N, S_N

def x_concat_one(x):

  # concatenate 1 to x vectors for w0
  return np.concatenate([np.ones(x.shape), x], axis = 1)

def predictive(Phi_test, m_N, S_N, beta):
  
  # calculate predictive distribution using the equation in Lecture 07, 15p.
  m = Phi_test.dot(m_N)
  var = 1 / beta + np.sum(Phi_test.dot(S_N) * Phi_test, axis=1)

  return m, var

alpha = 2.0
beta = 25.0
step = 2

Phi_test = x_concat_one(x_true.reshape(x_true.shape[0],1))

plt.figure(figsize=(15,15))
plt.subplots_adjust(hspace=0.4)

# As loop iterates, training samples will be added.
for i in range((noise_NUM//step)):
  X_N = x_n[0:i*step]
  T_N = t_n[0:i*step]

  Phi_N = x_concat_one(X_N.reshape(X_N.shape[0],1))
  m_N, S_N = posterior(Phi_N, T_N, alpha, beta)

  # this part draws the posterior probability
  plt.subplot(noise_NUM/step, 3, i*3 + 1)
  plt.title(f'Posterior probability (N = {i*step})')
  grid_x = grid_y = np.linspace(-1,1, 100)
  grid_flat = np.dstack(np.meshgrid(grid_x, grid_y)).reshape(-1,2)
  densities = stats.multivariate_normal.pdf(grid_flat, mean=m_N.ravel(), cov=S_N).reshape(100, 100)
  plt.imshow(densities, origin='lower', extent=(-1,1,-1,1))
  plt.scatter(w0, w1, marker='x', c='r', s=20, label='Truth')
  plt.xlabel('$w_0$')
  plt.ylabel('$w_1$')
  plt.legend()

  # sample data from the weight distribution, and predictive distribution.
  w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T
  y_samples = Phi_test.dot(w_samples)

  # draw lines for sampled data as well as the truth line.
  plt.subplot(noise_NUM/step, 3, i*3 + 2)
  plt.scatter(X_N, T_N, marker='o', c='k', s=20)
  plt.plot(x_true, y_true, 'k--', label='Truth')
  plt.plot(x_true, y_samples[:,0], 'r-', alpha=0.5, label='Posterior samples')
  for j in range(1, y_samples.shape[1]): 
    plt.plot(x_true, y_samples[:,j], 'r-', alpha=0.5)
  plt.xlabel('x')
  plt.ylabel('y')
  plt.ylim(-1.5, 1.5)
  plt.legend()

  # draw predictive distribution
  m_pred, var_pred = predictive(Phi_test, m_N, S_N, beta)

  plt.subplot(noise_NUM/step, 3, i*3 + 3)
  plt.scatter(X_N, T_N, marker='o', c='k', s=20)
  plt.plot(x_true, y_true, 'k--', label='Truth')
  y = y_true.ravel()
  std = np.sqrt(var_pred).ravel()
  plt.plot(x_true, m_pred, label='Prediction')
  plt.fill_between(x_true.ravel(), m_pred+std, m_pred-std, alpha=0.5, label='Uncertainty')
  plt.xlabel('x')
  plt.ylabel('y')
  plt.ylim(-1.5, 1.5)
  plt.legend()
